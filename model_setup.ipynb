{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc76a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# === Step 1: User Inputs ===\n",
    "genre = input(\"üéµ Enter genre (e.g., pop, rock, EDM): \")\n",
    "instruments = input(\"üé∏ Enter instruments (e.g., guitar, piano): \")\n",
    "mood = input(\"üåà Enter mood or theme (e.g., sad, uplifting, romantic): \")\n",
    "lyrics_prompt = input(\"üìù Describe what the lyrics should be about: \")\n",
    "\n",
    "# === Step 2: LangChain Prompt Template ===\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"genre\", \"instruments\", \"mood\", \"lyrics\"],\n",
    "    template=\"A {mood} {genre} song with {instruments}. Inspired by the theme: {lyrics}\"\n",
    ")\n",
    "\n",
    "# === Step 3: Create Final Prompt ===\n",
    "final_prompt = prompt_template.format(\n",
    "    genre=genre,\n",
    "    instruments=instruments,\n",
    "    mood=mood,\n",
    "    lyrics=lyrics_prompt\n",
    ")\n",
    "\n",
    "print(\"\\nüé§ Final Music Prompt:\")\n",
    "print(final_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd577cc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ba733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f14347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dataset_dir = \"song_lyrics_dataset/json1\"\n",
    "output_file = \"train.json\"\n",
    "\n",
    "with open(output_file, \"w\") as out_f:\n",
    "    for file_name in os.listdir(dataset_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(dataset_dir, file_name), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            artist = file_name.replace(\"Lyrics_\", \"\").replace(\".json\", \"\")\n",
    "            for song in data:\n",
    "                lyrics = song.get(\"lyrics\", \"\").strip()\n",
    "                title = song.get(\"title\", \"Untitled\")\n",
    "                if len(lyrics.split()) > 50:\n",
    "                    prompt = f\"Write a song like {artist}, titled '{title}'.\"\n",
    "                    formatted = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": prompt},\n",
    "                            {\"role\": \"assistant\", \"content\": lyrics}\n",
    "                        ]\n",
    "                    }\n",
    "                    out_f.write(json.dumps(formatted) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# MODEL\n",
    "base_model = \"mistralai/Mistral-7B-Instruct\"\n",
    "\n",
    "# Load tokenizer & model (with 4-bit quantization)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # important fix for padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"train.jsonl\")[\"train\"]\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(example):\n",
    "    prompt = example[\"messages\"][0][\"content\"]\n",
    "    response = example[\"messages\"][1][\"content\"]\n",
    "    full_prompt = f\"<s>[INST] {prompt} [/INST] {response}</s>\"\n",
    "    return tokenizer(full_prompt, truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-lyrics-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=True  # token-efficient\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./mistral-lyrics-finetuned\")\n",
    "tokenizer.save_pretrained(\"./mistral-lyrics-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch\n",
    "\n",
    "# === Load fine-tuned model ===\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./mistral-lyrics-finetuned\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mistral-lyrics-finetuned\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # to avoid padding issues\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"genre\", \"instruments\", \"mood\", \"lyrics\"],\n",
    "    template=\"A {mood} {genre} song with {instruments}. Inspired by the theme: {lyrics}\"\n",
    ")\n",
    "formatted_prompt = prompt_template.format(\n",
    "    genre=genre,\n",
    "    instruments=instruments,\n",
    "    mood=mood,\n",
    "    lyrics=lyrics_prompt\n",
    ")\n",
    "\n",
    "# === Step 3: Mistral Inference Function ===\n",
    "def generate_lyrics(full_prompt):\n",
    "    input_text = f\"<s>[INST] {full_prompt} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# === Step 4: Run Model ===\n",
    "lyrics_output = generate_lyrics(formatted_prompt)\n",
    "print(\"\\nüé§ Generated Lyrics:\\n\")\n",
    "print(lyrics_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3276ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "import torch\n",
    "\n",
    "# === Load fine-tuned Mistral model ===\n",
    "print(\"üîÅ Loading fine-tuned Mistral...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./mistral-lyrics-finetuned\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mistral-lyrics-finetuned\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === Load MusicGen model ===\n",
    "print(\"üîÅ Loading MusicGen...\")\n",
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')  # Use 'medium' or 'melody' for better quality\n",
    "music_model.set_generation_params(duration=15)  # seconds\n",
    "\n",
    "# === Step 2: Create prompt ===\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"genre\", \"instruments\", \"mood\", \"lyrics\"],\n",
    "    template=\"A {mood} {genre} song with {instruments}. Inspired by the theme: {lyrics}\"\n",
    ")\n",
    "formatted_prompt = prompt_template.format(\n",
    "    genre=genre,\n",
    "    instruments=instruments,\n",
    "    mood=mood,\n",
    "    lyrics=lyrics_prompt\n",
    ")\n",
    "\n",
    "# === Step 3: Generate lyrics ===\n",
    "def generate_lyrics(full_prompt):\n",
    "    input_text = f\"<s>[INST] {full_prompt} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "print(\"\\nüé§ Generating lyrics...\")\n",
    "lyrics = generate_lyrics(formatted_prompt)\n",
    "print(\"\\nüé∂ Lyrics:\\n\", lyrics)\n",
    "\n",
    "# === Step 4: Generate music from prompt ===\n",
    "print(\"\\nüéß Generating background music...\")\n",
    "wav = music_model.generate([formatted_prompt])  # input must be a list\n",
    "audio_write(\"generated_music\", wav[0].cpu(), sample_rate=32000)\n",
    "print(\"‚úÖ Music saved as 'generated_music.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "from bark import generate_audio, preload_models\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "import torch\n",
    "\n",
    "# === Load fine-tuned Mistral model ===\n",
    "print(\"üîÅ Loading fine-tuned Mistral...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./mistral-lyrics-finetuned\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mistral-lyrics-finetuned\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === Load MusicGen model ===\n",
    "print(\"üîÅ Loading MusicGen...\")\n",
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(duration=15)\n",
    "\n",
    "# === Load Bark (for vocals) ===\n",
    "print(\"üîÅ Loading Bark models...\")\n",
    "preload_models()\n",
    "\n",
    "# === Step 1: Take user input ===\n",
    "genre = input(\"üéµ Enter genre (e.g., pop, rock, EDM): \")\n",
    "instruments = input(\"üé∏ Enter instruments (e.g., guitar, piano): \")\n",
    "mood = input(\"üåà Enter mood or theme (e.g., sad, uplifting, romantic): \")\n",
    "lyrics_prompt = input(\"üìù Describe what the lyrics should be about: \")\n",
    "\n",
    "# === Step 2: Create full prompt ===\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"genre\", \"instruments\", \"mood\", \"lyrics\"],\n",
    "    template=\"A {mood} {genre} song with {instruments}. Inspired by the theme: {lyrics}\"\n",
    ")\n",
    "formatted_prompt = prompt_template.format(\n",
    "    genre=genre,\n",
    "    instruments=instruments,\n",
    "    mood=mood,\n",
    "    lyrics=lyrics_prompt\n",
    ")\n",
    "\n",
    "# === Step 3: Generate lyrics ===\n",
    "def generate_lyrics(full_prompt):\n",
    "    input_text = f\"<s>[INST] {full_prompt} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "print(\"\\nüé§ Generating lyrics...\")\n",
    "lyrics = generate_lyrics(formatted_prompt)\n",
    "print(\"\\nüé∂ Lyrics:\\n\", lyrics)\n",
    "\n",
    "# === Step 4: Generate background music ===\n",
    "print(\"\\nüéß Generating background music...\")\n",
    "music_wav = music_model.generate([formatted_prompt])\n",
    "audio_write(\"generated_music\", music_wav[0].cpu(), sample_rate=32000)\n",
    "print(\"‚úÖ Background music saved as 'generated_music.wav'\")\n",
    "\n",
    "# === Step 5: Generate vocals from lyrics using Bark ===\n",
    "print(\"\\nüéôÔ∏è Generating vocals from lyrics...\")\n",
    "vocal_array = generate_audio(lyrics)\n",
    "write_wav(\"vocal_output.wav\", rate=24000, data=vocal_array)\n",
    "print(\"‚úÖ Vocals saved as 'vocal_output.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# === Load both audio files ===\n",
    "bgm = AudioSegment.from_wav(\"generated_music.wav\")\n",
    "vocals = AudioSegment.from_wav(\"vocal_output.wav\")\n",
    "\n",
    "# === Adjust levels if needed ===\n",
    "bgm = bgm - 5  # reduce BGM volume to make vocals clearer\n",
    "\n",
    "# === Overlay vocals onto music ===\n",
    "final_mix = bgm.overlay(vocals, position=0)\n",
    "\n",
    "# === Export final mix ===\n",
    "final_mix.export(\"final_song.wav\", format=\"wav\")\n",
    "print(\"‚úÖ Final song saved as 'final_song.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdf009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load WAV file\n",
    "song = AudioSegment.from_wav(\"final_song.wav\")\n",
    "\n",
    "# Export as MP3\n",
    "song.export(\"final_song.mp3\", format=\"mp3\")\n",
    "print(\"‚úÖ Converted and saved as 'final_song.mp3'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
